{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "In this notebook, you'll practice (almost) everything you've learnt in the workshop. You're going to read in a bunch of documents, perform preprocessing, and then train and evaluate a text classifier.\n",
    "\n",
    "### Data\n",
    "\n",
    "I've downloaded the \"Blog Authorship\" corpus from from [here](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm). This is a corpus of 19,320 bloggers gathered from blogger.com in August 2004. The corpus has a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person. Each blog has been tagged with the blogger's (self-identified) gender, age, industry and astrological star sign. At a later time, I'd encourage you to read [the paper](http://u.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf) that describes the corpus.\n",
    "\n",
    "Each blog is in a separate xml file. The names of the file indicate the blogger id in the corpus, then their gender, age, industry and start sign. Within the xml file, there are two tags: date and post. We're going to ignore the date tag. All the data we want is in the post tag.\n",
    "\n",
    "### Task\n",
    "There are lots of things you could do with this, but we're going to try to build a classifier to predict an blogger's age bracket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import glob\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from string import punctuation\n",
    "from xml.etree import ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RAW_DATA_DIR = '../data/blogs'\n",
    "DATA_DIR = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data\n",
    "\n",
    "The first thing we want to do is read in all the data we'll need. We need both the text of the blog posts and the various attributes of the blogger that we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_properties_from_fname(fname):\n",
    "    fname = os.path.basename(fname)\n",
    "    return fname.split('.')\n",
    "\n",
    "def rounddown(n):\n",
    "    return math.floor(n/10) * 10\n",
    "\n",
    "def extract_age_from_fname(fname):\n",
    "    properties = extract_properties_from_fname(fname)\n",
    "    age = int(properties[2])\n",
    "    rounded_age = rounddown(age)\n",
    "    string_age = str(rounded_age) + 's'\n",
    "    return string_age\n",
    "\n",
    "def extract_gender_from_fname(fname):\n",
    "    properties = extract_properties_from_fname(fname)\n",
    "    gender = properties[1]\n",
    "    return gender\n",
    "\n",
    "def extract_id_from_fname(fname):\n",
    "    properties = extract_properties_from_fname(fname)\n",
    "    num = int(properties[0])\n",
    "    return num\n",
    "\n",
    "def extract_industry_from_fname(fname):\n",
    "    properties = extract_properties_from_fname(fname)\n",
    "    industry = properties[3]\n",
    "    return industry\n",
    "\n",
    "def extract_starsign_from_fname(fname):\n",
    "    properties = extract_properties_from_fname(fname)\n",
    "    starsign = properties[4]\n",
    "    return starsign\n",
    "\n",
    "def extract_all_text(fname):\n",
    "    e = ET.parse(fname)\n",
    "    root = e.getroot()\n",
    "    posts = root.findall('post')\n",
    "    text = [post.text for post in posts]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def extract_data(fname):\n",
    "    age = extract_age_from_fname(fname)\n",
    "    gender = extract_gender_from_fname(fname)\n",
    "    starsign = extract_starsign_from_fname(fname)\n",
    "    industry = extract_industry_from_fname(fname)\n",
    "    try:\n",
    "        text = extract_all_text(fname)\n",
    "    except ET.ParseError:\n",
    "        text = np.NaN\n",
    "    return age, gender, starsign, industry, text\n",
    "\n",
    "def load_blogs():\n",
    "    prepared_fname = os.path.join(DATA_DIR, 'blogs.csv')\n",
    "    if os.path.exists(prepared_fname):\n",
    "        return pd.read_csv(prepared_fname)\n",
    "    fname_pattern = os.path.join(RAW_DATA_DIR, '*.xml')\n",
    "    fnames = glob.glob(fname_pattern)\n",
    "    data = []\n",
    "    for fname in fnames:\n",
    "        data.append(extract_data(fname))\n",
    "    df = pd.DataFrame(data, columns=['age', 'gender', 'starsign', 'industry', 'text'])\n",
    "    df.dropna(how='any', inplace=True)\n",
    "    df.to_csv(prepared_fname, index=False)\n",
    "    return df\n",
    "\n",
    "data = load_blogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(data['text'])\n",
    "response = list(data['gender'])\n",
    "print(\"The first response is:\\n\", response[0])\n",
    "print(\"\\nAnd here's the associated text:\\n\", texts[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "Now you have two variables called `texts` and `response`. `texts` is a list of strings, where each string is the text contents of a single blog post. `response` is also a list of strings, where each string is a description of an attribute of the blogger who wrote that post. Before we can do anything, we'll have to clean up this data a little. The responses are ok, but the text data itself is pretty dirty.\n",
    "\n",
    "#### Challenge\n",
    "Your task now is to preprocess `texts` as much as you'd like. At the end of your preprocessing, we want to still have a list of strings for each blog post called `cleaned_texts`. That is, we want:\n",
    "\n",
    "`[\"this is the first blog post\", \"hello this is my second blog post\", ..., \"the final blog post\"]`.\n",
    "\n",
    "What this means is that if you decide to do any tokenization, or any steps that involve tokenization, you'll have to join the tokens back together so that each blog post is a string (not a list of strings itself).\n",
    "\n",
    "Here are some suggestions on what preprocessing you could do:\n",
    "- remove punctuation\n",
    "- lower case everything\n",
    "- remove extra whitespace\n",
    "- replace any URLs with something like \" URL \"\n",
    "- replace any digits with \" DIGIT \"\n",
    "- remove any stopwords\n",
    "- remove any words less than 3 characters in length\n",
    "- stem/lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return ''.join([ch for ch in text if ch not in punctuation])\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    whitespace_pattern = r'\\s+'\n",
    "    no_whitespace = re.sub(whitespace_pattern, ' ', text)\n",
    "    return no_whitespace.strip()\n",
    "\n",
    "def remove_url(text):\n",
    "    url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "    URL_SIGN = ' URL '\n",
    "    return re.sub(url_pattern, URL_SIGN, text)\n",
    "\n",
    "def remove_digits(text):\n",
    "    digit_pattern = '\\d+'\n",
    "    DIGIT_SIGN = ' DIGIT '\n",
    "    return re.sub(digit_pattern, DIGIT_SIGN, text)\n",
    "\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        return word_tokenize(text)\n",
    "    except:\n",
    "        return text.split()\n",
    "    \n",
    "stops = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokenized_text = tokenize(text)\n",
    "    no_stopwords = [token for token in tokenized_text if token not in stops]\n",
    "    return ' '.join(no_stopwords)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    tokenized_text = tokenize(text)\n",
    "    stems = [stemmer.stem(token) for token in tokenized_text]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_url(text)\n",
    "    text = remove_digits(text)\n",
    "    #text = remove_stopwords(text)\n",
    "    #text = stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = [clean(text) for text in texts]\n",
    "assert type(cleaned_texts) == type([]), \"cleaned_texts should be a list\"\n",
    "assert type(cleaned_texts[0]) == type(''), \"each element in cleaned_texts should be a string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM/TF-IDF\n",
    "\n",
    "#### Challenge\n",
    "Now let's take our list of strings `cleaned_texts` and turn it into a DTM, with either counts or TF-IDF scores. It's up to you which one you choose. Here's the documentation for the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and here's the documentation for [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). I'd suggest limited the `max_features` to 5000 and setting `binary=True`. Feel free to play around with other options too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=5000, binary=True)\n",
    "features = vectorizer.fit_transform(cleaned_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Now we're on to the actual classification step. The first thing we need to do here is split our data into a training and a test set. This is so we can evaluate the quality of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, response, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(X_train, y_train):\n",
    "    model = LogisticRegressionCV(Cs=5, penalty='l1', cv=3, solver='liblinear', refit=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def conmat(model, X_test, y_test):\n",
    "    \"\"\"Wrapper for sklearn's confusion matrix.\"\"\"\n",
    "    labels = model.classes_\n",
    "    y_pred = model.predict(X_test)\n",
    "    c = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(c, annot=True, fmt='d', \n",
    "                xticklabels=labels, \n",
    "                yticklabels=labels, \n",
    "                cmap=\"YlGnBu\", cbar=False)\n",
    "    plt.ylabel('Ground truth')\n",
    "    plt.xlabel('Prediction')\n",
    "    \n",
    "def test_model(model, X_train, y_train):\n",
    "    conmat(model, X_test, y_test)\n",
    "    print('Accuracy: ', model.score(X_test, y_test))\n",
    "    \n",
    "def interpret(vectorizer, model):\n",
    "    vocab = [(v,k) for k,v in vectorizer.vocabulary_.items()]\n",
    "    vocab = sorted(vocab, key=lambda x: x[0])\n",
    "    vocab = [word for num,word in vocab]\n",
    "    important = pd.DataFrame(model.coef_).T\n",
    "    if len(model.classes_) == 2:\n",
    "        important.columns = [model.classes_[0]]\n",
    "    else:\n",
    "        important.columns = model.classes_\n",
    "    important['word'] = vocab\n",
    "    return important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting what our model learnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important = interpret(vectorizer, model)\n",
    "important.sort_values(by='10s', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our trained model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_blog_posts = [\"I hate being a teenager so much! School is so boring! I don't even do my chemisty homework haha \\\n",
    "                  I just copy the soltuions from my friendz yay lol\",\n",
    "                  \"This is another post about my new job and life in this big city.\"]\n",
    "\n",
    "cleaned_new_blog_posts = [clean(post) for post in new_blog_posts]\n",
    "new_features = vectorizer.transform(cleaned_new_blog_posts)\n",
    "predictions = model.predict(new_features)\n",
    "list(zip(new_blog_posts, predictions))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
